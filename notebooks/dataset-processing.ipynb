{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create HuggingFace Dataset from Element Highlights\n",
    "\n",
    "This notebook processes element-highlights snapshots into a HuggingFace dataset with image-caption pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Libraries and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (3.3.2)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (0.29.2)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (11.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from aiohttp->datasets) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/rrweb-processing/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib tqdm datasets huggingface_hub pillow\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset, Image as DSImage, DatasetDict, Features, Value\n",
    "from huggingface_hub import login, HfApi, create_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "HF_USERNAME = \"Slyracoon23\"  # Your HuggingFace username\n",
    "DATASET_NAME = \"rrvideo-element-highlights\"  # Name for your dataset\n",
    "ELEMENT_HIGHLIGHTS_DIR = \"../element-highlights\"  # Path to element-highlights directory\n",
    "PROCESSED_DATA_DIR = \"processed_data\"  # Directory to store processed data\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(PROCESSED_DATA_DIR, \"images\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Login to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already logged in as: Slyracoon23\n",
      "Repository Slyracoon23/rrvideo-element-highlights is ready\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, whoami\n",
    "try:\n",
    "\n",
    "    # Try to get user info to check if already logged in\n",
    "    user_info = whoami()\n",
    "    print(f\"Already logged in as: {user_info['name']}\")\n",
    "except Exception:\n",
    "    print(\"Not logged in. Please login...\")\n",
    "    # Use the simpler login() function\n",
    "    try:\n",
    "        login()\n",
    "        print(\"Login successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during login: {e}\")\n",
    "        print(\"Please make sure you're logged in before proceeding.\")\n",
    "\n",
    "# Create repository if it doesn't exist\n",
    "try:\n",
    "    create_repo(\n",
    "        repo_id=f\"{HF_USERNAME}/{DATASET_NAME}\",\n",
    "        repo_type=\"dataset\",\n",
    "        exist_ok=True,\n",
    "        private=False\n",
    "    )\n",
    "    print(f\"Repository {HF_USERNAME}/{DATASET_NAME} is ready\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating repository: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption for google/gemini-2.0-flash-001:\n",
      "Here's a detailed description of the UI element within the red box:\n",
      "\n",
      "**Purpose:**\n",
      "\n",
      "This UI element represents one option for the user to choose as their workspace type. It's a \"Starter Workspace\" pre-configured for basic features and individual use. It is part of the Quickstart process which aims to simplify user setup of a workspace.\n",
      "\n",
      "**Appearance:**\n",
      "\n",
      "*   **Shape:** It appears to be structured as a Rounded rectangular tile.\n",
      "*   **Border:** Red border around the rectangular tile.\n",
      "*   **Content:**\n",
      "    *   An icon, an airplane icon on the top left.\n",
      "    *   A heading, \"Starter Workspace\", which indicates the type of workspace that will be configured.\n",
      "    *   A right arrow\n",
      "    *   Checkboxes: There are 3 checkboxes: Basic feautures, Ideal for individuals, Quick setup\n",
      "    *   Bullet-pointed list: A list of three features: Basic features, Ideal for individuals, Quick setup.\n",
      "*   **Visual Hierarchy:** The heading \"Starter Workspace\" is likely more prominent than the other text, making it the primary identifier.\n",
      "\n",
      "**Functionality:**\n",
      "\n",
      "*   **Selection:** By clicking on this container, the user signals their intent to create a \"Starter Workspace.\"\n",
      "*   **Navigation:** Selecting this option would likely trigger the next step in the Quickstart process (step 2), which is to \"Name your Workspace.\"\n",
      "\n",
      "In essence, this tile is a visually appealing and self-contained choice in a setup wizard. It informs the user about the type of workspace and its related features.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up OpenRouter API key\n",
    "OPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    print(\"Warning: OPENROUTER_API_KEY not found in environment variables\")\n",
    "\n",
    "def parse_snapshot_name(snapshot_dir):\n",
    "    \"\"\"Extract snapshot ID and timestamp from directory name\"\"\"\n",
    "    base_name = os.path.basename(snapshot_dir)\n",
    "    parts = base_name.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        try:\n",
    "            snapshot_id = int(parts[1])\n",
    "            timestamp = int(parts[2])\n",
    "            return {\n",
    "                \"snapshot_id\": snapshot_id,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"snapshot_name\": base_name\n",
    "            }\n",
    "        except ValueError:\n",
    "            print(f\"Could not parse snapshot name: {base_name}\")\n",
    "    return None\n",
    "\n",
    "def generate_image_hash(img_path):\n",
    "    \"\"\"Generate a hash for the image to prevent duplicates\"\"\"\n",
    "    try:\n",
    "        with open(img_path, 'rb') as f:\n",
    "            return hashlib.md5(f.read()).hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating hash for {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_element_metadata(img_path):\n",
    "    \"\"\"Extract basic metadata from the image\"\"\"\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        width, height = img.size\n",
    "        return {\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"aspect_ratio\": width / height if height > 0 else 0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata from {img_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def generate_caption(element_data, model=\"openai/gpt-4o\"):\n",
    "    \"\"\"Generate a caption for the element using OpenRouter AI API\"\"\"\n",
    "    import requests\n",
    "    import json\n",
    "    import base64\n",
    "    import os\n",
    "    \n",
    "    # Get image path and read the image\n",
    "    img_path = element_data.get(\"image_path\")\n",
    "    if not img_path or not os.path.exists(img_path):\n",
    "        # Fallback to basic caption if image not available\n",
    "        return generate_basic_caption(element_data)\n",
    "    \n",
    "    try:\n",
    "        # Read and encode the image\n",
    "        with open(img_path, \"rb\") as image_file:\n",
    "            base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        \n",
    "        # Create the API request\n",
    "        response = requests.post(\n",
    "            url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "                \"HTTP-Referer\": os.environ.get('YOUR_SITE_URL', 'http://localhost'),\n",
    "                \"X-Title\": os.environ.get('YOUR_SITE_NAME', 'UI Element Caption Generator'),\n",
    "                \"Content-Type\": \"application/json\",\n",
    "            },\n",
    "            data=json.dumps({\n",
    "                \"model\": model,  # Now configurable via parameter\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                # \"text\": \"Describe this UI element within the red box in detail. Include its purpose, appearance, and possible functionality.\"\n",
    "                                \"text\": \"Create a comprehensive caption for the UI element within the red box. Describe its structural components, text content, visual appearance, and overall functionality within the interface.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        result = response.json()\n",
    "        if 'choices' in result and len(result['choices']) > 0:\n",
    "            caption = result['choices'][0]['message']['content']\n",
    "            return caption\n",
    "        else:\n",
    "            print(f\"Error generating caption: No valid response from API\")\n",
    "            return generate_basic_caption(element_data)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating caption with OpenRouter: {e}\")\n",
    "        return generate_basic_caption(element_data)\n",
    "\n",
    "def generate_basic_caption(element_data):\n",
    "    \"\"\"Generate a basic caption as fallback\"\"\"\n",
    "    element_id = element_data.get(\"element_id\", \"unknown\")\n",
    "    snapshot_id = element_data.get(\"snapshot_id\", \"unknown\")\n",
    "    metadata = element_data.get(\"metadata\", {})\n",
    "    \n",
    "    # Basic caption\n",
    "    caption = f\"UI element {element_id} from snapshot {snapshot_id}\"\n",
    "    \n",
    "    # Add dimensions if available\n",
    "    if \"width\" in metadata and \"height\" in metadata:\n",
    "        caption += f\", dimensions {metadata['width']}x{metadata['height']} pixels\"\n",
    "    \n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing generate Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption for google/gemini-2.0-flash-001:\n",
      "UI element test_boardwalk from snapshot 1, dimensions 2560x1440 pixels\n"
     ]
    }
   ],
   "source": [
    "test_image_path=\"/Users/earlpotters/Documents/decipher/rrvideo/notebooks/processed_data/images/03a0a0b53d5ed3bffe2c6de7f6dee3a3.png\"\n",
    "# Create element_data dictionary for testing\n",
    "test_element_data = {\n",
    "    \"image_path\": test_image_path,\n",
    "    \"element_id\": \"test_boardwalk\",\n",
    "    \"snapshot_id\": 1,\n",
    "    \"metadata\": {\"width\": 2560, \"height\": 1440, \"aspect_ratio\": 2560 / 1440}\n",
    "}\n",
    "\n",
    "# Test generate_caption using the google/gemini-2.0-flash-001 model\n",
    "caption = generate_caption(test_element_data, model=\"google/gemini-2.0-flash-001\")\n",
    "print(\"Generated caption for google/gemini-2.0-flash-001:\")\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 snapshot directories\n",
      "Using the following models for captioning:\n",
      "- google/gemini-2.0-flash-001\n",
      "- qwen/qwen-vl-plus\n",
      "- amazon/nova-lite-v1\n",
      "- meta-llama/llama-3.2-11b-vision-instruct\n",
      "- openai/gpt-4o-mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d74b16bbbf4b48908aac6bb2277b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing snapshots:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 61 elements across 2 snapshots\n"
     ]
    }
   ],
   "source": [
    "# Define a global constant for the models to use for captioning\n",
    "MODELS = [\n",
    "    \"google/gemini-2.0-flash-001\",\n",
    "    \"qwen/qwen-vl-plus\",\n",
    "    \"amazon/nova-lite-v1\",\n",
    "    \"meta-llama/llama-3.2-11b-vision-instruct\",\n",
    "    \"openai/gpt-4o-mini\"\n",
    "]\n",
    "\n",
    "def process_snapshot(snapshot_dir):\n",
    "    \"\"\"Process a single snapshot directory and return image-caption pairs\"\"\"\n",
    "    snapshot_info = parse_snapshot_name(snapshot_dir)\n",
    "    if not snapshot_info:\n",
    "        return []\n",
    "    \n",
    "    # Get all element images in the directory (excluding originals)\n",
    "    element_images = [f for f in glob.glob(os.path.join(snapshot_dir, \"element_*.png\")) \n",
    "                      if not f.endswith(\"original.png\")]\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for img_path in element_images:\n",
    "        # Get element ID from filename\n",
    "        element_id = os.path.basename(img_path).replace(\"element_\", \"\").replace(\".png\", \"\")\n",
    "        \n",
    "        # Generate a hash for the image\n",
    "        img_hash = generate_image_hash(img_path)\n",
    "        if not img_hash:\n",
    "            continue\n",
    "        \n",
    "        # Copy image to processed directory with hash name\n",
    "        new_img_path = os.path.join(PROCESSED_DATA_DIR, \"images\", f\"{img_hash}.png\")\n",
    "        shutil.copy(img_path, new_img_path)\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = extract_element_metadata(img_path)\n",
    "        \n",
    "        # Prepare data entry\n",
    "        element_data = {\n",
    "            \"image_path\": new_img_path,\n",
    "            \"original_path\": img_path,\n",
    "            \"image_hash\": img_hash,\n",
    "            \"snapshot_id\": snapshot_info[\"snapshot_id\"],\n",
    "            \"timestamp\": snapshot_info[\"timestamp\"],\n",
    "            \"element_id\": element_id,\n",
    "            \"snapshot_name\": snapshot_info[\"snapshot_name\"],\n",
    "            \"metadata\": metadata,\n",
    "            \"captions\": {}  # Dictionary to store captions from different models\n",
    "        }\n",
    "        \n",
    "        # Generate captions using each model defined in MODELS\n",
    "        for model in MODELS:\n",
    "            caption = generate_caption(element_data, model=model)\n",
    "            element_data[\"captions\"][model] = caption\n",
    "        \n",
    "        data.append(element_data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_all_snapshots():\n",
    "    \"\"\"Process all snapshot directories and compile dataset\"\"\"\n",
    "    # Get all snapshot directories\n",
    "    snapshot_dirs = glob.glob(os.path.join(ELEMENT_HIGHLIGHTS_DIR, \"snapshot_*\"))\n",
    "    print(f\"Found {len(snapshot_dirs)} snapshot directories\")\n",
    "    \n",
    "    print(\"Using the following models for captioning:\")\n",
    "    for model in MODELS:\n",
    "        print(f\"- {model}\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for snapshot_dir in tqdm(snapshot_dirs, desc=\"Processing snapshots\"):\n",
    "        snapshot_data = process_snapshot(snapshot_dir)\n",
    "        all_data.extend(snapshot_data)\n",
    "    \n",
    "    print(f\"Processed {len(all_data)} elements across {len(snapshot_dirs)} snapshots\")\n",
    "    \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Save intermediate CSV for inspection\n",
    "    df.to_csv(os.path.join(PROCESSED_DATA_DIR, \"element_data.csv\"), index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process all snapshots\n",
    "df = process_all_snapshots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "Total number of elements: 61\n",
      "Number of unique snapshots: 2\n",
      "Number of unique image hashes: 61\n",
      "Found 0 duplicate images (based on hash)\n",
      "\n",
      "First row of the DataFrame:\n",
      "{'image_path': 'processed_data/images/dc49746d182ca196fefe2a1d4ef7fe99.png', 'original_path': '../element-highlights/snapshot_0_1739844581278/element_2.png', 'image_hash': 'dc49746d182ca196fefe2a1d4ef7fe99', 'snapshot_id': 0, 'timestamp': 1739844581278, 'element_id': '2', 'snapshot_name': 'snapshot_0_1739844581278', 'metadata': {'width': 1280, 'height': 720, 'aspect_ratio': 1.7777777777777777}, 'captions': {'google/gemini-2.0-flash-001': 'Here\\'s a description of the UI element within the red box:\\n\\n**Purpose:**\\n\\n*   This element represents the HTTP status code \"404,\" which is a standard error message indicating that the server cannot find the requested resource. It serves to immediately signify a page not found scenario to the user.\\n\\n**Appearance:**\\n\\n*   The numbers \"404\" are displayed in a bold, sans-serif font, making them visually prominent.\\n*   The numbers are black, contrasting strongly with the white background to grab the user\\'s attention. They are enclosed in a red box to make them more prominent.\\n\\n**Possible Functionality (In terms of UI/UX):**\\n\\n*   While the \"404\" element itself doesn\\'t directly have functionality, its presence triggers a specific user experience. It\\'s often part of a larger \"Page Not Found\" error page. When it appears, it prompts the user to understand that their request cannot be fulfilled at the given URL. It might lead them to:\\n    *   Double-check the URL for errors.\\n    *   Return to the homepage by clicking a \"Go Home\" button (as seen in the full image).\\n    *   Use a search feature to find the content they were looking for.\\n', 'qwen/qwen-vl-plus': \"I'm sorry, but as an AI language model, I don't have access to images or visual elements. Please provide me with more information or a description of the UI element within the red box so that I can assist you better.\", 'amazon/nova-lite-v1': \"The UI element in the red box is the '404 not found' error code displayed prominently on the screen. This code indicates that the content sought by the visitor is no longer available or cannot be accessed. It is likely a central and primary alert for the visitors regarding the issue. There is no interactivity or additional information available within this specific element.\\n\\nPurpose:\\nThe purpose of this element is to inform the browser window that the server is unable to find the resource requested by the user on their current request. It helps users understand that the page they are trying to visit is not present on the current server or web application.\\n\\nAppearance:\\n- It has a simple, rectangular shape that contains the numerical error code '404.'\\n- The font is bold, likely a sans-serif style, in black, contrasting well with the white background, ensuring it is the first thing visitors see.\\n- It is centered, so it remains noticeable without needing to scroll, allowing all users to see it immediately.\\n\\nOverall, the element serves as a simple yet effective means of communicating to users that the page they are seeking is unavailable in the current context.\", 'meta-llama/llama-3.2-11b-vision-instruct': 'The image depicts a simple error page, featuring a prominent \"404\" error notification at the center. The \"404\" is displayed in a large, black font within a red box, with a plain white background.\\n\\nBelow the \"404\" error notification, the phrase \"Page Not Found\" is written in smaller black text.\\n\\nAlthough there is a campaign pitch written on this page, it is too blurry to be legible.\\n\\nAt the bottom center of the image, a black button with white text reads \"Go Home.\" The overall content of this error page lacks detail but effectively communicates a clear message regarding its purpose.', 'openai/gpt-4o-mini': 'The UI element within the red box displays the number \"404,\" which signifies a \"404 Not Found\" error. This specific error code is commonly encountered on the web when a requested page is unavailable or has been moved to a different location on the server. \\n\\n### Appearance:\\n- **Font Style**: The number is presented in a bold, prominent typeface, likely to capture the user\\'s attention immediately.\\n- **Color and Background**: The number appears to be black text on a white background, surrounded by a thin red box which emphasizes its importance.\\n- **Size**: The large size of the number makes it easily noticeable, indicating a significant issue that the user needs to acknowledge.\\n\\n### Purpose:\\nThe primary purpose of this UI element is to communicate to users that they have navigated to a page that cannot be found on the website. It serves as an alert, guiding users to understand that their request has not been fulfilled due to the specific error.\\n\\n### Possible Functionality:\\n- **Visual Identification**: The \"404\" serves as a quick visual cue to help users immediately recognize the type of error they are facing.\\n- **Navigation Aid**: Following this message, there may be options (like a \"Go Home\" button) to redirect users back to a home page or a search feature to help them find the content they were looking for.\\n- **User Experience**: By presenting the error in a clear format, it helps minimize frustration and assists users in finding alternative routes to access information on the site. \\n\\nOverall, this UI element plays a crucial role in web navigation and user experience design by clearly indicating errors and guiding users forward.'}, 'width': 1280, 'height': 720, 'aspect_ratio': 1.7777777777777777}\n",
      "\n",
      "Missing values in each column:\n",
      "image_path       0\n",
      "original_path    0\n",
      "image_hash       0\n",
      "snapshot_id      0\n",
      "timestamp        0\n",
      "element_id       0\n",
      "snapshot_name    0\n",
      "metadata         0\n",
      "captions         0\n",
      "width            0\n",
      "height           0\n",
      "aspect_ratio     0\n",
      "dtype: int64\n",
      "\n",
      "Found 0 elements with invalid dimensions\n",
      "\n",
      "Summary statistics for dimensions:\n",
      "        width  height  aspect_ratio\n",
      "count    61.0    61.0  6.100000e+01\n",
      "mean   1280.0   720.0  1.777778e+00\n",
      "std       0.0     0.0  2.238873e-16\n",
      "min    1280.0   720.0  1.777778e+00\n",
      "25%    1280.0   720.0  1.777778e+00\n",
      "50%    1280.0   720.0  1.777778e+00\n",
      "75%    1280.0   720.0  1.777778e+00\n",
      "max    1280.0   720.0  1.777778e+00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>original_path</th>\n",
       "      <th>image_hash</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>element_id</th>\n",
       "      <th>snapshot_name</th>\n",
       "      <th>metadata</th>\n",
       "      <th>captions</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>aspect_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>processed_data/images/dc49746d182ca196fefe2a1d...</td>\n",
       "      <td>../element-highlights/snapshot_0_1739844581278...</td>\n",
       "      <td>dc49746d182ca196fefe2a1d4ef7fe99</td>\n",
       "      <td>0</td>\n",
       "      <td>1739844581278</td>\n",
       "      <td>2</td>\n",
       "      <td>snapshot_0_1739844581278</td>\n",
       "      <td>{'width': 1280, 'height': 720, 'aspect_ratio':...</td>\n",
       "      <td>{'google/gemini-2.0-flash-001': 'Here's a desc...</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>processed_data/images/55a0f8fc35aae4e05e865e95...</td>\n",
       "      <td>../element-highlights/snapshot_0_1739844581278...</td>\n",
       "      <td>55a0f8fc35aae4e05e865e959bc641f9</td>\n",
       "      <td>0</td>\n",
       "      <td>1739844581278</td>\n",
       "      <td>3</td>\n",
       "      <td>snapshot_0_1739844581278</td>\n",
       "      <td>{'width': 1280, 'height': 720, 'aspect_ratio':...</td>\n",
       "      <td>{'google/gemini-2.0-flash-001': 'Here's a deta...</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>processed_data/images/aef811ad4061f445231bb9f2...</td>\n",
       "      <td>../element-highlights/snapshot_0_1739844581278...</td>\n",
       "      <td>aef811ad4061f445231bb9f28a802c03</td>\n",
       "      <td>0</td>\n",
       "      <td>1739844581278</td>\n",
       "      <td>1</td>\n",
       "      <td>snapshot_0_1739844581278</td>\n",
       "      <td>{'width': 1280, 'height': 720, 'aspect_ratio':...</td>\n",
       "      <td>{'google/gemini-2.0-flash-001': 'Here's a deta...</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>processed_data/images/80021c881e81e4fbb33a0193...</td>\n",
       "      <td>../element-highlights/snapshot_0_1739844581278...</td>\n",
       "      <td>80021c881e81e4fbb33a01936cbebf2b</td>\n",
       "      <td>0</td>\n",
       "      <td>1739844581278</td>\n",
       "      <td>4</td>\n",
       "      <td>snapshot_0_1739844581278</td>\n",
       "      <td>{'width': 1280, 'height': 720, 'aspect_ratio':...</td>\n",
       "      <td>{'google/gemini-2.0-flash-001': 'Here's a brea...</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>processed_data/images/bb25d50e6e7a940d8b6697c4...</td>\n",
       "      <td>../element-highlights/snapshot_0_1739844581278...</td>\n",
       "      <td>bb25d50e6e7a940d8b6697c4603f20d8</td>\n",
       "      <td>0</td>\n",
       "      <td>1739844581278</td>\n",
       "      <td>5</td>\n",
       "      <td>snapshot_0_1739844581278</td>\n",
       "      <td>{'width': 1280, 'height': 720, 'aspect_ratio':...</td>\n",
       "      <td>{'google/gemini-2.0-flash-001': 'Here is a des...</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>processed_data/images/45af163c8448f0f03cc4c6e8...</td>\n",
       "      <td>../element-highlights/snapshot_445_17398455496...</td>\n",
       "      <td>45af163c8448f0f03cc4c6e8ddb49e70</td>\n",
       "      <td>445</td>\n",
       "      <td>1739845549649</td>\n",
       "      <td>68</td>\n",
       "      <td>snapshot_445_1739845549649</td>\n",
       "      <td>{'width': 1280, 'height': 720, 'aspect_ratio':...</td>\n",
       "      <td>{'google/gemini-2.0-flash-001': 'Here's a desc...</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>processed_data/images/b3ef84c076c8dcc64143d275...</td>\n",
       "      <td>../element-highlights/snapshot_445_17398455496...</td>\n",
       "      <td>b3ef84c076c8dcc64143d275f4ce1a6c</td>\n",
       "      <td>445</td>\n",
       "      <td>1739845549649</td>\n",
       "      <td>40</td>\n",
       "      <td>snapshot_445_1739845549649</td>\n",
       "      <td>{'width': 1280, 'height': 720, 'aspect_ratio':...</td>\n",
       "      <td>{'google/gemini-2.0-flash-001': 'Here's a deta...</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>processed_data/images/1353475dba79843fb73b5c45...</td>\n",
       "      <td>../element-highlights/snapshot_445_17398455496...</td>\n",
       "      <td>1353475dba79843fb73b5c45c2cf093f</td>\n",
       "      <td>445</td>\n",
       "      <td>1739845549649</td>\n",
       "      <td>54</td>\n",
       "      <td>snapshot_445_1739845549649</td>\n",
       "      <td>{'width': 1280, 'height': 720, 'aspect_ratio':...</td>\n",
       "      <td>{'google/gemini-2.0-flash-001': 'Here's a deta...</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>processed_data/images/2ac181b3ecf130bb601e5864...</td>\n",
       "      <td>../element-highlights/snapshot_445_17398455496...</td>\n",
       "      <td>2ac181b3ecf130bb601e5864c9f04b29</td>\n",
       "      <td>445</td>\n",
       "      <td>1739845549649</td>\n",
       "      <td>55</td>\n",
       "      <td>snapshot_445_1739845549649</td>\n",
       "      <td>{'width': 1280, 'height': 720, 'aspect_ratio':...</td>\n",
       "      <td>{'google/gemini-2.0-flash-001': 'Here's a deta...</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>processed_data/images/7b8ea83ced10a4698d3d6b76...</td>\n",
       "      <td>../element-highlights/snapshot_445_17398455496...</td>\n",
       "      <td>7b8ea83ced10a4698d3d6b765c99b499</td>\n",
       "      <td>445</td>\n",
       "      <td>1739845549649</td>\n",
       "      <td>41</td>\n",
       "      <td>snapshot_445_1739845549649</td>\n",
       "      <td>{'width': 1280, 'height': 720, 'aspect_ratio':...</td>\n",
       "      <td>{'google/gemini-2.0-flash-001': 'Here's a deta...</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_path  \\\n",
       "0   processed_data/images/dc49746d182ca196fefe2a1d...   \n",
       "1   processed_data/images/55a0f8fc35aae4e05e865e95...   \n",
       "2   processed_data/images/aef811ad4061f445231bb9f2...   \n",
       "3   processed_data/images/80021c881e81e4fbb33a0193...   \n",
       "4   processed_data/images/bb25d50e6e7a940d8b6697c4...   \n",
       "..                                                ...   \n",
       "56  processed_data/images/45af163c8448f0f03cc4c6e8...   \n",
       "57  processed_data/images/b3ef84c076c8dcc64143d275...   \n",
       "58  processed_data/images/1353475dba79843fb73b5c45...   \n",
       "59  processed_data/images/2ac181b3ecf130bb601e5864...   \n",
       "60  processed_data/images/7b8ea83ced10a4698d3d6b76...   \n",
       "\n",
       "                                        original_path  \\\n",
       "0   ../element-highlights/snapshot_0_1739844581278...   \n",
       "1   ../element-highlights/snapshot_0_1739844581278...   \n",
       "2   ../element-highlights/snapshot_0_1739844581278...   \n",
       "3   ../element-highlights/snapshot_0_1739844581278...   \n",
       "4   ../element-highlights/snapshot_0_1739844581278...   \n",
       "..                                                ...   \n",
       "56  ../element-highlights/snapshot_445_17398455496...   \n",
       "57  ../element-highlights/snapshot_445_17398455496...   \n",
       "58  ../element-highlights/snapshot_445_17398455496...   \n",
       "59  ../element-highlights/snapshot_445_17398455496...   \n",
       "60  ../element-highlights/snapshot_445_17398455496...   \n",
       "\n",
       "                          image_hash  snapshot_id      timestamp element_id  \\\n",
       "0   dc49746d182ca196fefe2a1d4ef7fe99            0  1739844581278          2   \n",
       "1   55a0f8fc35aae4e05e865e959bc641f9            0  1739844581278          3   \n",
       "2   aef811ad4061f445231bb9f28a802c03            0  1739844581278          1   \n",
       "3   80021c881e81e4fbb33a01936cbebf2b            0  1739844581278          4   \n",
       "4   bb25d50e6e7a940d8b6697c4603f20d8            0  1739844581278          5   \n",
       "..                               ...          ...            ...        ...   \n",
       "56  45af163c8448f0f03cc4c6e8ddb49e70          445  1739845549649         68   \n",
       "57  b3ef84c076c8dcc64143d275f4ce1a6c          445  1739845549649         40   \n",
       "58  1353475dba79843fb73b5c45c2cf093f          445  1739845549649         54   \n",
       "59  2ac181b3ecf130bb601e5864c9f04b29          445  1739845549649         55   \n",
       "60  7b8ea83ced10a4698d3d6b765c99b499          445  1739845549649         41   \n",
       "\n",
       "                 snapshot_name  \\\n",
       "0     snapshot_0_1739844581278   \n",
       "1     snapshot_0_1739844581278   \n",
       "2     snapshot_0_1739844581278   \n",
       "3     snapshot_0_1739844581278   \n",
       "4     snapshot_0_1739844581278   \n",
       "..                         ...   \n",
       "56  snapshot_445_1739845549649   \n",
       "57  snapshot_445_1739845549649   \n",
       "58  snapshot_445_1739845549649   \n",
       "59  snapshot_445_1739845549649   \n",
       "60  snapshot_445_1739845549649   \n",
       "\n",
       "                                             metadata  \\\n",
       "0   {'width': 1280, 'height': 720, 'aspect_ratio':...   \n",
       "1   {'width': 1280, 'height': 720, 'aspect_ratio':...   \n",
       "2   {'width': 1280, 'height': 720, 'aspect_ratio':...   \n",
       "3   {'width': 1280, 'height': 720, 'aspect_ratio':...   \n",
       "4   {'width': 1280, 'height': 720, 'aspect_ratio':...   \n",
       "..                                                ...   \n",
       "56  {'width': 1280, 'height': 720, 'aspect_ratio':...   \n",
       "57  {'width': 1280, 'height': 720, 'aspect_ratio':...   \n",
       "58  {'width': 1280, 'height': 720, 'aspect_ratio':...   \n",
       "59  {'width': 1280, 'height': 720, 'aspect_ratio':...   \n",
       "60  {'width': 1280, 'height': 720, 'aspect_ratio':...   \n",
       "\n",
       "                                             captions  width  height  \\\n",
       "0   {'google/gemini-2.0-flash-001': 'Here's a desc...   1280     720   \n",
       "1   {'google/gemini-2.0-flash-001': 'Here's a deta...   1280     720   \n",
       "2   {'google/gemini-2.0-flash-001': 'Here's a deta...   1280     720   \n",
       "3   {'google/gemini-2.0-flash-001': 'Here's a brea...   1280     720   \n",
       "4   {'google/gemini-2.0-flash-001': 'Here is a des...   1280     720   \n",
       "..                                                ...    ...     ...   \n",
       "56  {'google/gemini-2.0-flash-001': 'Here's a desc...   1280     720   \n",
       "57  {'google/gemini-2.0-flash-001': 'Here's a deta...   1280     720   \n",
       "58  {'google/gemini-2.0-flash-001': 'Here's a deta...   1280     720   \n",
       "59  {'google/gemini-2.0-flash-001': 'Here's a deta...   1280     720   \n",
       "60  {'google/gemini-2.0-flash-001': 'Here's a deta...   1280     720   \n",
       "\n",
       "    aspect_ratio  \n",
       "0       1.777778  \n",
       "1       1.777778  \n",
       "2       1.777778  \n",
       "3       1.777778  \n",
       "4       1.777778  \n",
       "..           ...  \n",
       "56      1.777778  \n",
       "57      1.777778  \n",
       "58      1.777778  \n",
       "59      1.777778  \n",
       "60      1.777778  \n",
       "\n",
       "[61 rows x 12 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total number of elements: {len(df)}\")\n",
    "print(f\"Number of unique snapshots: {df['snapshot_id'].nunique()}\")\n",
    "print(f\"Number of unique image hashes: {df['image_hash'].nunique()}\")\n",
    "\n",
    "# Extract width and height from metadata for easier access\n",
    "df['width'] = df['metadata'].apply(lambda x: x.get('width', 0))\n",
    "df['height'] = df['metadata'].apply(lambda x: x.get('height', 0))\n",
    "df['aspect_ratio'] = df['metadata'].apply(lambda x: x.get('aspect_ratio', 0))\n",
    "\n",
    "# Check for duplicate images\n",
    "duplicates = df[df.duplicated(subset=['image_hash'], keep='first')]\n",
    "print(f\"Found {len(duplicates)} duplicate images (based on hash)\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "if len(duplicates) > 0:\n",
    "    df = df.drop_duplicates(subset=['image_hash'], keep='first')\n",
    "    print(f\"After removing duplicates: {len(df)} elements\")\n",
    "\n",
    "# Display the first row of the DataFrame to understand its structure\n",
    "print(\"\\nFirst row of the DataFrame:\")\n",
    "print(df.iloc[0].to_dict())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Check for invalid dimensions (zero or negative width/height)\n",
    "invalid_dimensions = df[(df['width'] <= 0) | (df['height'] <= 0)]\n",
    "print(f\"\\nFound {len(invalid_dimensions)} elements with invalid dimensions\")\n",
    "\n",
    "# Remove elements with invalid dimensions if any\n",
    "if len(invalid_dimensions) > 0:\n",
    "    df = df[(df['width'] > 0) & (df['height'] > 0)]\n",
    "    print(f\"After removing invalid dimensions: {len(df)} elements\")\n",
    "\n",
    "# Display summary statistics for numerical columns\n",
    "print(\"\\nSummary statistics for dimensions:\")\n",
    "print(df[['width', 'height', 'aspect_ratio']].describe())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create and Upload HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and uploading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e676edcd6ab841aeb0f488a0950d1e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "  train: 48 examples\n",
      "  validation: 6 examples\n",
      "  test: 7 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff699f0059a043a3b90dab402d0ec2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747b6eb8f35f48c7b141a7c46584dbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d24207a84a4effbb0141a8c8f10da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635e09ec21854c4192c1996411eab437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03aa8396b8f247689cb3c2887296d529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db344e11dfe4f8e85ea36b56d47d845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c836ed27c8f48c0be9f6ea535b27f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e61db0d876b4672a2d11d3d20b30163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f6cddd2811498c91bc4c6ea3315922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c44956ff0604ee89ea6633fa47a4fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset uploaded to https://huggingface.co/datasets/Slyracoon23/rrvideo-element-highlights\n"
     ]
    }
   ],
   "source": [
    "def create_and_upload_dataset(df):\n",
    "    \"\"\"Create HuggingFace dataset and upload to Hub\"\"\"\n",
    "    import json\n",
    "    from datasets import Dataset, DatasetDict, Features, Value, Image as DSImage\n",
    "    from PIL import Image\n",
    "\n",
    "    # Convert metadata dict to string to avoid serialization issues\n",
    "    df['metadata_str'] = df['metadata'].apply(json.dumps)\n",
    "    \n",
    "    # Extract width, height, and aspect_ratio from the metadata dictionary into separate columns\n",
    "    df['width'] = df['metadata'].apply(lambda m: m.get(\"width\") if isinstance(m, dict) else None)\n",
    "    df['height'] = df['metadata'].apply(lambda m: m.get(\"height\") if isinstance(m, dict) else None)\n",
    "    df['aspect_ratio'] = df['metadata'].apply(lambda m: m.get(\"aspect_ratio\") if isinstance(m, dict) else None)\n",
    "    \n",
    "    # Reset the index to avoid including it in the dataset\n",
    "    df_reset = df.reset_index(drop=True)\n",
    "    dataset = Dataset.from_pandas(df_reset)\n",
    "    \n",
    "    # Define features with updated \"captions\" field (a dict of captions per model)\n",
    "    features = Features({\n",
    "        \"image\": DSImage(),\n",
    "        \"captions\": {\n",
    "            \"google/gemini-2.0-flash-001\": Value(\"string\"),\n",
    "            \"qwen/qwen-vl-plus\": Value(\"string\"),\n",
    "            \"amazon/nova-lite-v1\": Value(\"string\"),\n",
    "            \"meta-llama/llama-3.2-11b-vision-instruct\": Value(\"string\"),\n",
    "            \"openai/gpt-4o-mini\": Value(\"string\")\n",
    "        },\n",
    "        \"snapshot_id\": Value(\"int64\"),\n",
    "        \"timestamp\": Value(\"int64\"),\n",
    "        \"element_id\": Value(\"string\"),\n",
    "        \"image_hash\": Value(\"string\"),\n",
    "        \"snapshot_name\": Value(\"string\"),\n",
    "        \"metadata_str\": Value(\"string\"),\n",
    "        \"width\": Value(\"int64\"),\n",
    "        \"height\": Value(\"int64\"),\n",
    "        \"aspect_ratio\": Value(\"float32\")\n",
    "    })\n",
    "    \n",
    "    # Function to convert image path to an actual image and include all fields\n",
    "    def process_example(example):\n",
    "        image = Image.open(example[\"image_path\"])\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"captions\": example[\"captions\"],\n",
    "            \"snapshot_id\": example[\"snapshot_id\"],\n",
    "            \"timestamp\": example[\"timestamp\"],\n",
    "            \"element_id\": example[\"element_id\"],\n",
    "            \"image_hash\": example[\"image_hash\"],\n",
    "            \"snapshot_name\": example[\"snapshot_name\"],\n",
    "            \"metadata_str\": example[\"metadata_str\"],\n",
    "            \"width\": example[\"width\"],\n",
    "            \"height\": example[\"height\"],\n",
    "            \"aspect_ratio\": example[\"aspect_ratio\"]\n",
    "        }\n",
    "    \n",
    "    # Apply the transformation; remove columns that are no longer needed\n",
    "    dataset = dataset.map(\n",
    "        process_example,\n",
    "        remove_columns=[\"image_path\", \"original_path\", \"metadata\"],\n",
    "        features=features\n",
    "    )\n",
    "    \n",
    "    # Split into train/validation/test sets (80/10/10 split)\n",
    "    splits = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    test_valid = splits[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "    \n",
    "    # Create a DatasetDict with the splits\n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": splits[\"train\"],\n",
    "        \"validation\": test_valid[\"train\"],\n",
    "        \"test\": test_valid[\"test\"]\n",
    "    })\n",
    "    \n",
    "    # Print split information\n",
    "    print(\"Dataset splits:\")\n",
    "    for split, subset in dataset_dict.items():\n",
    "        print(f\"  {split}: {len(subset)} examples\")\n",
    "    \n",
    "    # Push to the HuggingFace Hub\n",
    "    dataset_dict.push_to_hub(\n",
    "        f\"{HF_USERNAME}/{DATASET_NAME}\",\n",
    "        private=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset uploaded to https://huggingface.co/datasets/{HF_USERNAME}/{DATASET_NAME}\")\n",
    "    return dataset_dict\n",
    "\n",
    "# Create and upload dataset\n",
    "print(\"Creating and uploading dataset...\")\n",
    "dataset_dict = create_and_upload_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Dataset Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset card uploaded to https://huggingface.co/datasets/Slyracoon23/rrvideo-element-highlights\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_card():\n",
    "    \"\"\"Create a dataset card with information about the dataset\"\"\"\n",
    "    card = f\"\"\"\n",
    "---\n",
    "language:\n",
    "- en\n",
    "license: cc-by-4.0\n",
    "task_categories:\n",
    "- image-classification\n",
    "- image-to-text\n",
    "---\n",
    "\n",
    "# Dataset Card for {DATASET_NAME}\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "This dataset contains UI element images extracted from web snapshots, along with captions describing each element.\n",
    "\n",
    "### Dataset Summary\n",
    "\n",
    "The dataset contains {len(df)} unique UI elements from {df['snapshot_id'].nunique()} different snapshots. Each element is associated with a caption describing its basic properties.\n",
    "\n",
    "### Dataset Statistics\n",
    "\n",
    "- Total elements: {len(df)}\n",
    "- Unique snapshots: {df['snapshot_id'].nunique()}\n",
    "- Average element width: {df['width'].mean():.2f} pixels\n",
    "- Average element height: {df['height'].mean():.2f} pixels\n",
    "- Average aspect ratio: {df['aspect_ratio'].mean():.2f}\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "### Data Fields\n",
    "\n",
    "- **image**: The UI element image\n",
    "- **caption**: A description of the element using several models\n",
    "- **snapshot_id**: The snapshot identifier\n",
    "- **timestamp**: When the snapshot was taken\n",
    "- **element_id**: Identifier for the specific element\n",
    "- **width**: The width of the element in pixels\n",
    "- **height**: The height of the element in pixels\n",
    "- **aspect_ratio**: The aspect ratio of the element\n",
    "\n",
    "### Data Splits\n",
    "\n",
    "- Train: {len(dataset_dict['train'])} examples (80%)\n",
    "- Validation: {len(dataset_dict['validation'])} examples (10%)\n",
    "- Test: {len(dataset_dict['test'])} examples (10%)\n",
    "\"\"\"\n",
    "    \n",
    "    # Write dataset card to file\n",
    "    readme_path = os.path.join(PROCESSED_DATA_DIR, \"README.md\")\n",
    "    with open(readme_path, \"w\") as f:\n",
    "        f.write(card)\n",
    "    \n",
    "    # Upload to HuggingFace Hub\n",
    "    api = HfApi()\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=readme_path,\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=f\"{HF_USERNAME}/{DATASET_NAME}\",\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset card uploaded to https://huggingface.co/datasets/{HF_USERNAME}/{DATASET_NAME}\")\n",
    "\n",
    "# Create and upload dataset card\n",
    "create_dataset_card()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, num_samples=5):\n",
    "    \"\"\"Visualize random samples from the dataset\"\"\"\n",
    "    if len(dataset) < num_samples:\n",
    "        num_samples = len(dataset)\n",
    "    \n",
    "    indices = np.random.randint(0, len(dataset), num_samples)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=(10, num_samples * 3))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        sample = dataset[int(idx)]\n",
    "        image = sample[\"image\"]\n",
    "        caption = sample[\"caption\"]\n",
    "        \n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f\"Caption: {caption}\", fontsize=10)\n",
    "        axes[i].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PROCESSED_DATA_DIR, \"sample_visualizations.png\"))\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples from the training set\n",
    "print(\"Visualizing samples from the training set...\")\n",
    "visualize_samples(dataset_dict[\"train\"])\n",
    "\n",
    "print(\"\\nProcess complete! Your dataset is now available at:\")\n",
    "print(f\"https://huggingface.co/datasets/{HF_USERNAME}/{DATASET_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rrweb-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
